{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'geoj.geojson'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c80aadbfc205>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeojson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'geoj.geojson'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mgj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeojson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'geoj.geojson'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import folium\n",
    "\n",
    "# !pip3 install geopy\n",
    "from geopy.distance import distance\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# !pip3 install iteround\n",
    "from iteround import saferound\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def flatten_list(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def pairwise(iterable):\n",
    "    it = iter(iterable)\n",
    "    a = next(it, None)\n",
    "    for b in it:\n",
    "        yield [a, b]\n",
    "        a = b\n",
    "        \n",
    "def pairwise_list(li, endpoint_unique=False, include_end=False):\n",
    "    li = list(pairwise(li))\n",
    "    if include_end:\n",
    "        for i in range(1,len(li)):\n",
    "            li[i][0] = li[i][0]+1\n",
    "    elif endpoint_unique:\n",
    "        for i in range(0,len(li)-1):\n",
    "            li[i][1] = li[i][1]-1\n",
    "    return li\n",
    "\n",
    "\n",
    "def angleDiff(sourceA,targetA):\n",
    "    a = targetA - sourceA\n",
    "    a = (a + 180) % 360 - 180\n",
    "    return a\n",
    "\n",
    "def unique_num(x):\n",
    "    return len(set(x))\n",
    "\n",
    "def add_degree_to_azimuth(current, change):\n",
    "    assert(abs(change)<180)\n",
    "    output = current+change\n",
    "    if output<-180:\n",
    "        output = 180-(-output-180)\n",
    "    elif output>180:\n",
    "        output = -180+(output-180)\n",
    "    if output == -180:\n",
    "        output = -output\n",
    "    return output\n",
    "    \n",
    "import geojson\n",
    "with open('geoj.geojson') as f:\n",
    "    gj = geojson.load(f)\n",
    "\n",
    "# print(gj.keys())\n",
    "# print(gj['type'],gj['name'],gj['crs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_to_map_zoom_mapping = interp1d([0,100,15000],[20,16,10])\n",
    "\n",
    "def inspect_street(street_name, return_df = False):\n",
    "    \n",
    "    try:\n",
    "\n",
    "        part = df[df.street_name==street_name]\n",
    "        part['length'] = part['segment_lengths'].apply(sum)\n",
    "        street_total_length = part.length.sum()\n",
    "\n",
    "        street_points_list = part.coordinates.tolist()\n",
    "\n",
    "        all_streets_center_loc = np.mean([np.mean(li, axis=0).tolist() for li in street_points_list],axis=0)\n",
    "\n",
    "        my_map = folium.Map(location=all_streets_center_loc,\n",
    "                                zoom_start= float(length_to_map_zoom_mapping(street_total_length)),\n",
    "                                tiles=\"CartoDB dark_matter\")\n",
    "\n",
    "        for street_points in street_points_list:\n",
    "\n",
    "            folium.PolyLine(street_points, color=\"red\", weight=1).add_to(my_map)\n",
    "\n",
    "        if return_df:\n",
    "            return my_map, part\n",
    "\n",
    "        return my_map\n",
    "\n",
    "    except:\n",
    "        \n",
    "        print('Street Not Found: \"'+street_name+'\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data for a segment of street from the raw GeoJson file\n",
    "street_rows = []\n",
    "for shape in gj['features']:\n",
    "    properties_temp = shape['properties']\n",
    "    coordinates = shape['geometry']['coordinates'][0]\n",
    "    coordinates = np.flip(np.array(coordinates)).tolist()\n",
    "    properties_temp.update({'coordinates':coordinates})\n",
    "    street_rows.append(properties_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas dataframe containing the raw data\n",
    "\n",
    "df = pd.DataFrame(street_rows)\n",
    "\n",
    "df = df.applymap(lambda x: np.nan if x==None or x=='no name' else x)\n",
    "\n",
    "df.drop('Shape_Leng',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# L_f_add is the building number on the Front of the Left side of this street segment\n",
    "# R_t_add is the building number on the Tail of the Right side of this street segment\n",
    "# The first four fields are like these\n",
    "\n",
    "# Prefix and Suffix are N S E W direction, \n",
    "# Pretype and Type are street types,\n",
    "# Name and type collectively define the address, but sometimes it disagrees with the Full_name\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most frequent values for the first four fields, and see their patterns, this is how I guessed the meaning of these fields\n",
    "\n",
    "top_k = 10\n",
    "top_starts = pd.concat([df[col].value_counts()[:top_k].reset_index()['index'] for col in df.columns[:4]], axis=1)\n",
    "top_starts.columns = df.columns[:4]\n",
    "top_starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out unreliable entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out: Building number on the L and R side change in different order/direction\n",
    "# It's okay if building number grows on both L and R sides, or if building number shrinks on both L and R sides\n",
    "criteria_1 = ((np.sign(df['L_t_add']-df['L_f_add'])*np.sign(df['R_t_add']-df['R_f_add']))>=0)\n",
    "\n",
    "# Filter out: All building number start/end is zero\n",
    "criteria_2 = (df['L_t_add']+df['L_f_add']+df['R_t_add']+df['R_f_add']>0)\n",
    "\n",
    "# Filter out: Starts and ends of L and R differs too much\n",
    "criteria_3 = (~((np.abs(df['L_f_add']-df['R_f_add'])>30)|(np.abs(df['L_t_add']-df['R_t_add'])>30))&(df['L_t_add']*df['L_f_add']*df['R_t_add']*df['R_f_add']>0))\n",
    "\n",
    "df = df[ criteria_1 & criteria_2 & criteria_3 ].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0,0)     (10,10)  (15,15)\n",
    "# 1/2         4        5/6\n",
    "# 1-----------4---------6\n",
    "\n",
    "# 10 = 0 + 4/6*(15-0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['min_building_num'] = df[df.columns[:4]].apply(lambda row: min([x for x in row.tolist() if x>0]), axis=1)\n",
    "df['max_building_num'] = df[df.columns[:4]].apply(lambda row: max([x for x in row.tolist() if x>0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1                   99\n",
    "# 2        50         100  \n",
    "\n",
    "# 50/(100-1) ~= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 3 5 7 9\n",
    "# 2 4 6 8 10\n",
    "\n",
    "# 2 4 6 8 10\n",
    "# 1 3 5 7 9\n",
    "\n",
    "#         7?\n",
    "# 1----------10\n",
    "#         7?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['odd_on'] = df['L_f_add'].apply(lambda x: 'left' if x%2==1 else 'right')\n",
    "\n",
    "df.odd_on.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Type'] = df[['Pre_type','Type']].apply(lambda row: row['Type'] if not isinstance(row['Pre_type'],str) else row['Pre_type'] if not isinstance(row['Type'],str) or row['Type'] in row['Pre_type'] else row['Type'] if row['Pre_type'] in row['Type'] else row['Pre_type']+' '+row['Type'], axis=1).fillna('').apply(str.strip)\n",
    "\n",
    "Type_mapping = {'St': 'Street', 'Ave': 'Avenue', 'Pl': 'Place', 'Ln': 'Lane', 'Aly': 'Alley', 'Sq': 'Square', 'Rd': 'Road', 'Park': 'Park', 'Dr': 'Driveway', 'Sl': 'Slip', 'Row': 'Row', 'Ter': 'Terrace', 'Ct': 'Street', 'Wy': 'Way'}\n",
    "\n",
    "df['Type'] = df['Type'].apply(lambda x: Type_mapping[x] if x in Type_mapping.keys() else x)\n",
    "\n",
    "Type_mapping_for_full_name = {' St': ' Street', ' Ave': ' Avenue', ' Pl': ' Place', ' Ln': ' Lane', ' Aly': ' Alley', ' Sq': ' Square', ' Rd': ' Road', ' Park': ' Park', ' Dr': ' Driveway', ' Sl': ' Slip', ' Row': ' Row', ' Ter': ' Terrace', ' Ct': ' Street', ' Wy': ' Way'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_if_endswith(x,dic = Type_mapping_for_full_name):\n",
    "    if isinstance(x,str):\n",
    "        for k in Type_mapping_for_full_name.keys():\n",
    "            if x.endswith(k):\n",
    "                x = x.replace(k, Type_mapping_for_full_name[k])\n",
    "                break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Full_name'] = df['Full_name'].apply(replace_if_endswith)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Prefix and Suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(df[(df.Prefix.notnull())&(df.Suffix.isnull())])==0)\n",
    "\n",
    "# There is no case where Prefix is available while Suffix is missing, in other words, Suffix is always the more complete source\n",
    "\n",
    "# df[(df.Prefix.isnull())&(df.Suffix.notnull())]\n",
    "pd.concat([df.Prefix.value_counts(),df.Suffix.value_counts()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['street_name_contains_number'] = df['Name'].apply(lambda x: len(re.findall('\\d+',x))>0 if isinstance(x,str) else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Suffix'] = df['Suffix'].fillna('')\n",
    "df['Type'] = df['Type'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If Name field is not available, use Full_name\n",
    "# Else If Street Name contains number, then put direction indicator (NSWE) in the front (E 14th Street), otherwise put it in the end (Washington Park S).\n",
    "\n",
    "df['street_name'] = df.apply(lambda row: row['Full_name'] if not isinstance(row['Name'],str) else row['Suffix']+' '+row['Name']+' '+row['Type'] if row['street_name_contains_number'] else row['Name']+' '+row['Type']+' '+row['Suffix'] ,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the streets that building numbers are in opposite orders, reverse the coordinates' order, keep rest the same\n",
    "# VERY TRICKY, BUT IT WORKS, CONTACT TIM IF YOU ARE INTERESTED\n",
    "\n",
    "df.loc[df['L_f_add']>df['L_t_add'],'coordinates'] = df.loc[df['L_f_add']>df['L_t_add'],'coordinates'].apply(lambda li: li[::-1])\n",
    "\n",
    "df = df.drop(['L_f_add', 'L_t_add', 'R_f_add', 'R_t_add', 'Prefix', 'Pre_type','Name', 'Suffix', 'Full_name', 'City', 'State', 'Type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove blank space at rears\n",
    "\n",
    "df.street_name = df.street_name.apply(str.strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geographiclib.geodesic import Geodesic\n",
    "geod = Geodesic.WGS84\n",
    "# object for calculating distance and direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate information of a street segment based on coordinates\n",
    "\n",
    "def process_list_of_points(list_of_points):\n",
    "\n",
    "    segment_lengths = []\n",
    "    directions = []\n",
    "    for i in range(len(list_of_points)-1):\n",
    "        f_pt = list_of_points[i]\n",
    "        t_pt = list_of_points[i+1]\n",
    "        segment_lengths.append(distance(f_pt,t_pt).m)\n",
    "        directions.append(geod.Inverse(*t_pt, *f_pt)['azi1'])\n",
    "\n",
    "    returning_segment_lengths = segment_lengths[:]\n",
    "    returning_directions = directions[:]\n",
    "    \n",
    "    segment_lengths = np.array(segment_lengths)\n",
    "    directions = np.array(directions)\n",
    "    \n",
    "    mu,sig = np.median(directions),np.std(directions)\n",
    "    non_outlier_mask = (directions < mu+2*sig)&(directions > mu-2*sig)\n",
    "    if sum(non_outlier_mask)>0:\n",
    "        directions = directions[non_outlier_mask].copy()\n",
    "        segment_lengths = segment_lengths[non_outlier_mask].copy()\n",
    "    \n",
    "    weighted_average_direction = np.average(directions,weights=segment_lengths)\n",
    "\n",
    "    return returning_segment_lengths,returning_directions,weighted_average_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['segment_lengths'],df['directions'],df['weighted_avg_direction'] = zip(*df['coordinates'].apply(process_list_of_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(['street_name','min_building_num']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some street segments overlap with each other, we remove all street segments that have such conflicts\n",
    "# Fix min_building_num\n",
    "\n",
    "non_overlapping_range_df = pd.DataFrame()\n",
    "\n",
    "for street_name in df.street_name.unique().tolist():\n",
    "    \n",
    "    data = df[df.street_name==street_name]\n",
    "\n",
    "    data['prev_segment_max_building_num'] = data.max_building_num.shift(1)\n",
    "\n",
    "    data['min_building_num'] = data.apply(lambda row: row['min_building_num'] if np.isnan(row['prev_segment_max_building_num']) else row['min_building_num'] if row['min_building_num']>row['prev_segment_max_building_num'] else row['prev_segment_max_building_num']+1  , axis=1).apply(int)\n",
    "\n",
    "    data = data.drop('prev_segment_max_building_num',axis=1)\n",
    "    \n",
    "    non_overlapping_range_df = non_overlapping_range_df.append(data, ignore_index=True)\n",
    "    \n",
    "# print(sum(non_overlapping_range_df.max_building_num > non_overlapping_range_df.min_building_num))\n",
    "non_overlapping_range_df = non_overlapping_range_df[non_overlapping_range_df.max_building_num > non_overlapping_range_df.min_building_num].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Now there is no overlapping building num range problem\n",
    "\n",
    "data = non_overlapping_range_df.copy()\n",
    "\n",
    "data['building_num_range'] = data[['min_building_num','max_building_num']].apply(lambda row: list(range(row['min_building_num'],row['max_building_num'])),axis=1)\n",
    "\n",
    "overlapping_range_detection = data.groupby('street_name').agg({'building_num_range':list})\n",
    "\n",
    "overlapping_range_detection['building_num_range'] = overlapping_range_detection['building_num_range'].apply(flatten_list)\n",
    "\n",
    "overlapping_range_detection['building_num_range'] = overlapping_range_detection['building_num_range'].apply(lambda li: Counter(li))\n",
    "\n",
    "overlapping_range_detection['building_num_range'] = overlapping_range_detection['building_num_range'].apply(lambda counter: [k for k, v in counter.items() if v > 1])\n",
    "\n",
    "overlapping_range_detection['building_num_range'].apply(len).value_counts()\n",
    "\n",
    "assert(len( overlapping_range_detection['building_num_range'][overlapping_range_detection['building_num_range'].apply(len)>0] )==0) \n",
    "\n",
    "# # # Check satisfied\n",
    "\n",
    "df = non_overlapping_range_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['building_num_range_length'] = df['max_building_num'] - df['min_building_num']\n",
    "\n",
    "df['max_building_num_end_points'] = (df.segment_lengths.apply(lambda li: np.array(li)/np.sum(li))*df['building_num_range_length']).apply(lambda li: saferound(li, places=0)).apply(lambda li: np.cumsum(li).astype(np.int0))  +  df['min_building_num']\n",
    "\n",
    "df['building_num_points'] = df.apply(lambda row: [row['min_building_num']] + row['max_building_num_end_points'].tolist(), axis=1)\n",
    "\n",
    "df = df.drop('max_building_num_end_points',axis=1)\n",
    "\n",
    "assert(sum(df.building_num_points.apply(len) != df.coordinates.apply(len)) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_building_num, max_building_num:[20, 57]\n",
    "# building_num_points:[20, 43, 57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For street parts that have more than one segment, the endpoints are not listed in the same order as other parts. \n",
    "# For example, the last (or southeast end) of Pike street has two shape segments, and that part's points are listed in reverse order/direction compared to the other street parts. \n",
    "# After some exploration, it turns out that we need to reverse the list before using pairwise_list, so as to reflect the order of the longer sequence\n",
    "\n",
    "df['coordinates_segments'] = df['coordinates'].apply(lambda li: li[::-1]).apply(pairwise_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordinates:[point1, point2, point3] \n",
    "# coordinates_segments:[[point1, point2],   [point2, point3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['building_num_segments'] = df['building_num_points'].apply(lambda li: pairwise_list(li, endpoint_unique=True, include_end=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building_num_points:[20, 43, 57] \n",
    "# building_num_segments:[[20, 43],   [44, 57]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.DataFrame(zip(np.repeat(  df.street_name.values, df['coordinates_segments'].apply(len)  ),\n",
    "np.repeat(  df.odd_on.values, df['coordinates_segments'].apply(len)  ),\n",
    "np.repeat(  df.weighted_avg_direction.values, df['coordinates_segments'].apply(len)  ),\n",
    "flatten_list((df.building_num_segments.tolist())),\n",
    "flatten_list(df.coordinates_segments.tolist()),\n",
    "flatten_list((df.segment_lengths.tolist())),\n",
    "flatten_list((df.directions.tolist()))), columns = ['street_name','odd_on','avg_direction','building_num_range','start_end_coordinates','segment_length','segment_direction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full['building_num_range_length'] = full['building_num_range'].apply(lambda x: x[1]-x[0]+1)\n",
    "# print(sum(full['building_num_range_length']<=0))\n",
    "\n",
    "full = full[full['building_num_range_length']>0].reset_index(drop=True)\n",
    "\n",
    "full['avg_length_per_building'] = full['segment_length']/full['building_num_range_length']\n",
    "\n",
    "full['direction_deviation'] = full[['segment_direction','avg_direction']].apply(lambda row: angleDiff(row['segment_direction'],row['avg_direction']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------100m------------------#\n",
    "#  H       H    H           H        H     #\n",
    "# On average, each House occupies 100/5=20 meters\n",
    "\n",
    "plt.hist(full.avg_length_per_building,bins=60)\n",
    "avg_length_per_building_thres = np.percentile(full.avg_length_per_building,99)\n",
    "print(f'The buildings on the 99% of the streets occupy fewer than {avg_length_per_building_thres:.0f} meters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remove houses that are 3 times as wise as the 99% threshold\n",
    "full = full[full.avg_length_per_building<avg_length_per_building_thres * 3].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Different segments of a street should put odd building numbers on the same side\n",
    "# For the anomaly, we inspect them and decide how to fix them\n",
    "\n",
    "left_right_inconsistent_correction = full.groupby('street_name').agg({'odd_on':Counter})['odd_on'][full.groupby('street_name').agg({'odd_on':Counter})['odd_on'].apply(len)>1]\n",
    "left_right_inconsistent_correction.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If one kind of labels are ten times as many as the other one, we think the other label is a mistake in entry or our preprocessing\n",
    "# by this criteria, we will set \"1st Avenue\" to be all left, \"6th Avenue\" to be all left.\n",
    "\n",
    "full.loc[(full.street_name=='1st Avenue') | (full.street_name=='6th Avenue'), 'odd_on'] = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------#\n",
    "    # distance from center to one side = 5 meters\n",
    "# Street\n",
    "\n",
    "#-------------------------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------#\n",
    "\n",
    "    # distance from center to one side = 10 meters\n",
    "\n",
    "# Avenue\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# Once we know the location of center of the street, then SHIFT\n",
    "#      --- to the right or the left (depending on \"odd_on\" and the building number's odd/even status)\n",
    "#      --- by ? meters depending which type of road it is\n",
    "\n",
    "\n",
    "valid_road_type = {'Way', 'Street', 'Driveway', 'River', 'Square', 'Place', 'Front', 'Terrace', 'Slip', 'Island', 'Row', 'Alley', 'Road', 'Pier', 'Lane', 'Broadway', 'Park', 'Avenue'}\n",
    "\n",
    "full['road_type'] = full.street_name.apply(lambda x: re.sub('\\s+[N|E|S|W]$','',re.sub('\\s+\\d+','',x))).apply(lambda x: x.split()[-1] if x.split()[-1] in valid_road_type else 'Generic')\n",
    "\n",
    "full['offset_from_road_center'] = full['road_type'].apply(lambda x: 10 if x in ['Avenue','Broadway'] else 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.street_name = full.street_name.apply(lambda x: ' '.join(x.strip().split()))\n",
    "\n",
    "# Fix after manual inspection\n",
    "full.street_name = full.street_name.apply(lambda x: 'East Broadway' if x=='Broadway E' else 'West Broadway' if x=='Broadway W' else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets = full.street_name.drop_duplicates().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voila!\n",
    "# full.to_csv('_1880_streets_full.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualization # FOLIUM\n",
    "\n",
    "colors = ['red','orange','green','blue','purple']\n",
    "\n",
    "center_loc = np.mean(flatten_list(full.start_end_coordinates.tolist()),axis=0).tolist()\n",
    "\n",
    "my_map = folium.Map(location=center_loc,zoom_start= 13,tiles=\"CartoDB dark_matter\")\n",
    "\n",
    "\n",
    "# [ '6th Avenue', '7th Avenue']\n",
    "# [s for s in full.street_name.drop_duplicates().tolist() if s.endswith(' Avenue')]\n",
    "\n",
    "for i in range(len(streets)):\n",
    "    \n",
    "    street_name = streets[i]\n",
    "    \n",
    "    street_points = flatten_list(full[full.street_name == street_name].start_end_coordinates.tolist())\n",
    "\n",
    "    folium.PolyLine(street_points, color=colors[i%len(colors)], weight=1).add_to(my_map)\n",
    "    \n",
    "#     for pt in street_points:\n",
    "#         folium.Marker(location=pt,popup='%f, %f' % tuple(pt)).add_to(my_map)\n",
    "\n",
    "# del my_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = pd.read_csv('datasets/1910_New York_Manhattan Ward 7.csv')\n",
    "\n",
    "geo = geo[['RecordId','Street Name','House Number','Street Name New','House Number New','Address','Lat','Lon','Enumeration District Number','Ward of City']]\n",
    "\n",
    "geo['Street Name New'] = geo['Street Name New'].apply(lambda x: 'Rutgers Street' if x.startswith('Rutgers') else x)\n",
    "\n",
    "geo = geo.drop_duplicates(subset=geo.columns[1:].tolist(), keep='last')[geo.columns[1:]].reset_index()\n",
    "\n",
    "geo['Number of Residents'] = geo['index'].diff().fillna(geo['index'][0]).apply(int)\n",
    "\n",
    "geo = geo.drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(', '.join(sorted(geo['Street Name New'].drop_duplicates().tolist())))\n",
    "print()\n",
    "print(f\"{np.mean(geo['Street Name New'].isin(streets))*100:.1f}% of the streets in geo are covered in our 1880 address database.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = sorted(geo.Address.drop_duplicates().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates_from_details(target_building_num,building_num_range,start_end_coordinates,segment_direction,odd_on,offset_from_road_center):\n",
    "    \n",
    "    is_odd = target_building_num%2==1\n",
    "    \n",
    "    if (building_num_range[1] - building_num_range[0])==0:\n",
    "        street_center_position = np.mean(start_end_coordinates,axis=0).tolist()\n",
    "    else:\n",
    "        f_pt_proportion = (target_building_num - building_num_range[0])/(building_num_range[1] - building_num_range[0])\n",
    "        t_pt_proportion = 1 - f_pt_proportion\n",
    "        street_center_position = np.average(np.array(start_end_coordinates), weights = (f_pt_proportion,t_pt_proportion), axis=0).tolist()\n",
    "\n",
    "    offset_direction = add_degree_to_azimuth(segment_direction,-90) if ((odd_on=='left' and is_odd) or (odd_on=='right' and not is_odd))  else add_degree_to_azimuth(segment_direction,90)\n",
    "\n",
    "    target_position = geod.Direct(*street_center_position, offset_direction, offset_from_road_center)\n",
    "    target_point = list([target_position['lat2'],target_position['lon2']])\n",
    "\n",
    "    return target_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an address, first choose the relevant data based on street name\n",
    "# Then find which segment of the street is relevant baesd on the building number\n",
    "# After that, calculate the proportional location based on the building number and the start and end building numbers of the segment\n",
    "# Calculate the geolocation of the center of street at that location\n",
    "# Shift the location to one side by a proper distance\n",
    "# Return the final coordinates\n",
    "\n",
    "def get_addr_coordinates(addr):\n",
    "    target_street_name, target_building_num = ' '.join(addr.split()[:-1]), addr.split()[-1]\n",
    "    target_building_num = int(target_building_num)\n",
    "    matched = full[full.street_name==target_street_name].copy().apply(lambda row: get_coordinates_from_details(target_building_num, row['building_num_range'],row['start_end_coordinates'],row['segment_direction'],row['odd_on'],row['offset_from_road_center']) if target_building_num>= row['building_num_range'][0] and target_building_num <= row['building_num_range'][1] else np.nan, axis=1).dropna()\n",
    "    if len(matched)>0:\n",
    "        return tuple(matched.tolist()[0])\n",
    "    return (np.nan,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses_coordinates = []\n",
    "for addr in addresses:\n",
    "    addresses_coordinates.append(get_addr_coordinates(addr))\n",
    "\n",
    "addr_to_coordinates = pd.DataFrame(zip(addresses,addresses_coordinates),columns=['Address','Coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_geo = pd.merge(geo,addr_to_coordinates)\n",
    "\n",
    "new_geo = new_geo.rename(columns = {'Lat':'API Lat','Lon':'API Lon'})\n",
    "\n",
    "new_geo['GIS Lat'],new_geo['GIS Lon'] = zip(*new_geo['Coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nPreviously, {(new_geo.drop_duplicates(subset=['Address'])['API Lat'].value_counts()[new_geo.drop_duplicates(subset=['Address'])['API Lat'].value_counts()>1].sum()/len(geo.drop_duplicates(subset=['Address'])))*100:.1f}% of the addresses have the same API lat,lon coordinate with somewhere else.\\n\")\n",
    "print(f\"In comparison, {(new_geo.drop_duplicates(subset=['Address'])['GIS Lat'].value_counts()[new_geo.drop_duplicates(subset=['Address'])['GIS Lat'].value_counts()>1].sum()/len(geo.drop_duplicates(subset=['Address'])))*100:.1f}% of the addresses have the same GIS lat,lon coordinate with somewhere else.\\n\")\n",
    "print(f\"In terms of coverage, the API method has {new_geo['API Lat'].isnull().mean()*100:.01f}% missing values, while the GIS method has {new_geo['GIS Lat'].isnull().mean()*100:.01f}% missing values.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_new_geo = new_geo[new_geo.Coordinates!=(np.nan,np.nan)].copy()\n",
    "\n",
    "max_num_of_res = valid_new_geo['Number of Residents'].max()\n",
    "\n",
    "list_of_points = valid_new_geo.Coordinates.tolist()\n",
    "\n",
    "residents_at_points = valid_new_geo['Number of Residents'].tolist()\n",
    "\n",
    "max_r = (0.5+np.log(max(residents_at_points))**1.2)\n",
    "\n",
    "center_loc = np.mean( list_of_points ,axis=0).tolist()\n",
    "\n",
    "my_map = folium.Map(location=center_loc,zoom_start=15)# ,tiles=\"CartoDB dark_matter\")\n",
    "\n",
    "\n",
    "plot_streets = valid_new_geo['Street Name New'].drop_duplicates().tolist()\n",
    "for s in plot_streets:\n",
    "\n",
    "    line_weight = 8 if s.endswith('Avenue') or s.endswith('Broadway') else 5\n",
    "    street_points = full[full.street_name==s].start_end_coordinates.tolist()\n",
    "    folium.PolyLine(street_points, color='dimgrey', weight=line_weight, alpha = 0.1, popup=s).add_to(my_map)\n",
    "    \n",
    "\n",
    "for i in range(len(list_of_points)):\n",
    "    \n",
    "    pt = list_of_points[i]\n",
    "    num_of_res = residents_at_points[i]\n",
    "        \n",
    "    r = 0.5+np.log(num_of_res)**1.2\n",
    "    \n",
    "    color_ratio = r/max_r\n",
    "    rgb_tuple = (int(255/2 + 255/2*color_ratio),int(0),int(255/2 - 255/2*color_ratio))\n",
    "    hex_code = '#%02x%02x%02x' % rgb_tuple\n",
    "    \n",
    "    folium.Circle(location=pt,radius=r,color=hex_code,alpha=0.3 ).add_to(my_map)\n",
    "    \n",
    "\n",
    "\n",
    "my_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_map.save('demo.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point order problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this_street = '10th Avenue'\n",
    "# full[full.street_name==this_street]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full[full.street_name==this_street]['start_end_coordinates'].apply(lambda li: li[0][0]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full[full.street_name==this_street]['start_end_coordinates'].apply(lambda li: li[0][1]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full[full.street_name==this_street].segment_direction.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# street_points_list = full[full.street_name==this_street].start_end_coordinates.tolist() \n",
    "\n",
    "# ### Visual inspect\n",
    "\n",
    "# colors = ['red','orange','green','blue','purple','white','brown']\n",
    "\n",
    "# my_map = folium.Map(location=np.mean(flatten_list(street_points_list),axis=0).tolist(),\n",
    "#                         zoom_start= 17 ,\n",
    "#                         tiles=\"CartoDB dark_matter\")\n",
    "\n",
    "# for i in range(len(street_points_list)):\n",
    "#     street_points = street_points_list[i]\n",
    "#     # print(i,street_points)\n",
    "#     folium.PolyLine(street_points, color=colors[i%len(colors)], weight=(1+i%len(colors))).add_to(my_map)\n",
    "\n",
    "# #[[40.710512, -73.992112], [40.710864, -73.99232], [40.711312, -73.992416]]\n",
    "    \n",
    "# my_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# street_points_list = raw[raw['Name']==this_street.split()[0]].sort_values('L_f_add').reset_index(drop=True).coordinates.tolist()\n",
    "\n",
    "\n",
    "# ### Visual inspect\n",
    "\n",
    "# center = np.mean(flatten_list(street_points_list),axis=0).tolist()\n",
    "\n",
    "# colors = ['red','orange','green','blue','purple','white','brown']\n",
    "\n",
    "# my_map = folium.Map(location=center,\n",
    "#                         zoom_start= 17 ,\n",
    "#                         tiles=\"CartoDB dark_matter\")\n",
    "\n",
    "# for i in range(len(street_points_list)):\n",
    "#     street_points = street_points_list[i]\n",
    "#     # print(i,street_points)\n",
    "#     folium.PolyLine(street_points, color=colors[i%len(colors)], weight=(1+i%len(colors))).add_to(my_map)\n",
    "\n",
    "# my_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END** Point order problem <br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old Geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## old_geo = pd.read_csv('/Users/timsmac/Desktop/us_census_data/aggregate/Geoloaded_1910_New York_Manhattan Ward 7.csv')\n",
    "\n",
    "## old_geo = old_geo[['Street Name','House Number','Street Name New', 'House Number New', 'Address', 'Lon', 'Lat', 'Enumeration District Number', 'Ward of City']]\n",
    "\n",
    "## old_geo = old_geo.drop_duplicates(['Street Name','House Number'])\n",
    "\n",
    "## old_geo.to_csv('_old_geo_for_comparison.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END** Old Geo<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# li = [street_center_position,target_point]\n",
    "\n",
    "# my_map = folium.Map(location=np.mean(li,axis=0).tolist(), zoom_start= 20, tiles=\"CartoDB dark_matter\")\n",
    "\n",
    "# for i in range(len(li)):\n",
    "#     pt = li[i]\n",
    "#     folium.Circle(pt, radius=2,color=colors[i%len(colors)]).add_to(my_map)\n",
    "    \n",
    "# my_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Check and verify that there is overlapping building num range problem\n",
    "\n",
    "# data = df.copy()\n",
    "\n",
    "# data['building_num_range'] = df[['min_building_num','max_building_num']].apply(lambda row: list(range(row['min_building_num'],row['max_building_num'])),axis=1)\n",
    "\n",
    "# overlapping_range_detection = data.groupby('street_name').agg({'building_num_range':list})\n",
    "\n",
    "# overlapping_range_detection['building_num_range'] = overlapping_range_detection['building_num_range'].apply(flatten_list)\n",
    "\n",
    "# overlapping_range_detection['building_num_range'] = overlapping_range_detection['building_num_range'].apply(lambda li: Counter(li))\n",
    "\n",
    "# overlapping_range_detection['building_num_range'] = overlapping_range_detection['building_num_range'].apply(lambda counter: [k for k, v in counter.items() if v > 1])\n",
    "\n",
    "# overlapping_range_detection['building_num_range'].apply(len).value_counts()\n",
    "\n",
    "# overlapping_range_detection['building_num_range'][overlapping_range_detection['building_num_range'].apply(len)>0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Fix max_building_num\n",
    "\n",
    "# non_overlapping_range_df = pd.DataFrame()\n",
    "\n",
    "# for street_name in df.street_name.unique().tolist():\n",
    "    \n",
    "#     data = df[df.street_name==street_name]\n",
    "\n",
    "#     data['next_segment_min_building_num'] = data.min_building_num.shift(-1)\n",
    "\n",
    "#     data['max_building_num'] = data.apply(lambda row: row['max_building_num'] if np.isnan(row['next_segment_min_building_num']) else row['max_building_num'] if row['max_building_num']<row['next_segment_min_building_num'] else row['next_segment_min_building_num']-1  , axis=1).apply(int)\n",
    "\n",
    "#     data = data.drop('next_segment_min_building_num',axis=1)\n",
    "    \n",
    "#     non_overlapping_range_df = non_overlapping_range_df.append(data, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Visual inspect\n",
    "\n",
    "# street_points_list = df.coordinates.tolist()\n",
    "\n",
    "# all_streets_center_loc = np.mean([np.mean(li, axis=0).tolist() for li in street_points_list],axis=0)\n",
    "\n",
    "# my_map = folium.Map(location=all_streets_center_loc,\n",
    "#                         zoom_start= 13 ,\n",
    "#                         tiles=\"CartoDB dark_matter\")\n",
    "\n",
    "# for street_points in street_points_list:\n",
    "\n",
    "#     folium.PolyLine(street_points, color=\"red\", weight=1).add_to(my_map)\n",
    "\n",
    "# my_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob('1910MAN/*')\n",
    "\n",
    "# df = pd.read_csv('1910MAN/1910MANstCLN_W7.csv')\n",
    "\n",
    "# geo_relevant_cols = ['DataID', 'RecordId', 'HouseHoldId', 'Street Name', 'St_MOD',\n",
    "#        'ADDY_CLN', 'ZIP', 'City', 'State', 'Country', 'House Number', 'HoNo_1',\n",
    "#        'Dwelling Number','Enumeration District Number', 'Ward of City']\n",
    "\n",
    "# df = df[geo_relevant_cols]\n",
    "\n",
    "# df.ZIP.value_counts()\n",
    "\n",
    "# df.RecordId.nunique()\n",
    "\n",
    "# found = pd.read_csv('1910MAN/1910MANstCLN_W7_found.csv')[['RecordId','Longitude', 'Latitude', 'Side']]\n",
    "\n",
    "# found\n",
    "\n",
    "# found.Side.value_counts()\n",
    "\n",
    "# fdf = df[df.RecordId.isin( set(found.RecordId.tolist()) )]\n",
    "\n",
    "# fdf['House Number'].value_counts()\n",
    "\n",
    "# fdf.St_MOD.value_counts()\n",
    "\n",
    "# ndf = df[~df.RecordId.isin( set(found.RecordId.tolist()) )]\n",
    "\n",
    "# ndf['House Number'].value_counts()\n",
    "\n",
    "# ndf.St_MOD.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# address = 'Macombs Place 16'\n",
    "\n",
    "# street_name = ' '.join(address.split()[:-1])\n",
    "# building_number = address.split()[-1]\n",
    "\n",
    "# # np.cumsum(segment_lengths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bit0209044da7c742dab8314e0953beb93b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
